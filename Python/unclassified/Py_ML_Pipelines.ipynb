{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines with Scikit-Learn\n",
    "\n",
    "[Managing Machine Learning: Workflows with Scikit-learn Pipelines Part 1: A Gentle Introduction](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html)\n",
    "\n",
    "Data collection, cleaning and preparation is a critical step in the machine learning process. It is also a time-consuming and error-prone task. \n",
    "\n",
    "Some transformations and estimators can be chained together to form a pipeline. This is a convenient way to keep all the steps in one place and avoid data leakage.\n",
    "\n",
    "* Convenience - coherent, easy to understand workflow\n",
    "* Enforcing workflow steps - no data leakage\n",
    "* Reproducible\n",
    "* Value in persistence\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will do: \n",
    "\n",
    "Build 3 pipelines for the same dataset wiht different estimators (classification algorithms), using default hyperparameters:\n",
    "* Logistic regression\n",
    "* Support Vector Machine\n",
    "* Decison Tree\n",
    "\n",
    "Transforms:\n",
    "* Feature scaling\n",
    "* Dimensionality reduction with PCA\n",
    "\n",
    "Final Estimators\n",
    "\n",
    "Scoring tet data\n",
    "Compare pipeline model accuracies\n",
    "Identify \"best\" model (highest accuracy)\n",
    "Persist entire pipeline of \"best\" model\n",
    "\n",
    "All done with the `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some pipelines\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', svm.SVC(random_state=42))])\n",
    "\t\t\t\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "pipelines = [pipe_lr, pipe_svm, pipe_dt]\n",
    "\t\t\t\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Machine', 2: 'Decision Tree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipelines\n",
    "for pipe in pipelines:\n",
    "\tpipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.900\n",
      "Support Vector Machine pipeline test accuracy: 0.900\n",
      "Decision Tree pipeline test accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracies\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tprint('%s pipeline test accuracy: %.3f' % (pipe_dict[idx], val.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with best accuracy: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Identify the most accurate model on test data\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tif val.score(X_test, y_test) > best_acc:\n",
    "\t\tbest_acc = val.score(X_test, y_test)\n",
    "\t\tbest_pipe = val\n",
    "\t\tbest_clf = idx\n",
    "print('Classifier with best accuracy: %s' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Logistic Regression pipeline to file\n"
     ]
    }
   ],
   "source": [
    "# Save pipeline to file\n",
    "joblib.dump(best_pipe, 'best_pipeline.pkl', compress=1)\n",
    "print('Saved %s pipeline to file' % pipe_dict[best_clf])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipelines Part 2: Integrating Grid Search](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html)\n",
    "\n",
    "Grid search attempts to optimise model hyperparameter combinations.  \n",
    "\n",
    "Exhaustive grid search (as opposed to alternate hyperparameter combination optimisation like randomised optimisation) tests and compares all possible combinations - massively computationally expensive and time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.867\n",
      "\n",
      "Model hyperparameters:\n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct pipeline\n",
    "pipe = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Pipeline test accuracy\n",
    "print('Test accuracy: %.3f' % pipe.score(X_test, y_test))\n",
    "\n",
    "# Pipeline estimator params; estimator is stored as step 3 ([2]), second item ([1])\t\t\t\n",
    "print('\\nModel hyperparameters:\\n', pipe.steps[2][1].get_params())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding grid search to the pipeline\n",
    "\n",
    "Above is a decision tree estimator so we can optimise the following: \n",
    "\n",
    "* `criterion` - evaluates quality of the split (gini impurity or information gain (entropy))\n",
    "* `min_samples_leaf` - minimum number of samples required for a valid leaf node; we will use the integer range 1 to 5\n",
    "* `max_depth` - maximum depth of the tree, using range 1-5\n",
    "min_samples_split - minimum number of samples required to split an internal node (non-leaf node); we will use the integer range 1 to 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.917\n",
      "\n",
      "Best params:\n",
      " {'clf__criterion': 'gini', 'clf__max_depth': 2, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct pipeline\n",
    "pipe = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "param_range = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Set grid search params\n",
    "grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "\t\t'clf__min_samples_leaf': param_range,\n",
    "\t\t'clf__max_depth': param_range,\n",
    "\t\t'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "# Construct grid search\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "\t\t\tparam_grid=grid_params,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10)\n",
    "\n",
    "# Fit using grid search\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipelines Part 3: Multiple Models, Pipelines, and Grid Searches](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-3.html)\n",
    "\n",
    "Grid search to optimise models from different estimators, then compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Logistic Regression\n",
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.917\n",
      "Test set accuracy score for best params: 0.967 \n",
      "\n",
      "Estimator: Logistic Regression w/PCA\n",
      "Best params: {'clf__C': 0.5, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Best training accuracy: 0.858\n",
      "Test set accuracy score for best params: 0.933 \n",
      "\n",
      "Estimator: Random Forest\n",
      "Best params: {'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 7}\n",
      "Best training accuracy: 0.950\n",
      "Test set accuracy score for best params: 1.000 \n",
      "\n",
      "Estimator: Random Forest w/PCA\n",
      "Best params: {'clf__criterion': 'gini', 'clf__max_depth': 3, 'clf__min_samples_leaf': 3, 'clf__min_samples_split': 8}\n",
      "Best training accuracy: 0.908\n",
      "Test set accuracy score for best params: 0.900 \n",
      "\n",
      "Estimator: Support Vector Machine\n",
      "Best params: {'clf__C': 3, 'clf__kernel': 'linear'}\n",
      "Best training accuracy: 0.967\n",
      "Test set accuracy score for best params: 0.967 \n",
      "\n",
      "Estimator: Support Vector Machine w/PCA\n",
      "Best params: {'clf__C': 2, 'clf__kernel': 'rbf'}\n",
      "Best training accuracy: 0.925\n",
      "Test set accuracy score for best params: 0.900 \n",
      "\n",
      "Classifier with best test set accuracy: Random Forest\n",
      "\n",
      "Saved Random Forest grid search pipeline to file: best_gs_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct some pipelines\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_lr_pca = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_rf = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "pipe_rf_pca = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('clf', svm.SVC(random_state=42))])\n",
    "\n",
    "pipe_svm_pca = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', svm.SVC(random_state=42))])\n",
    "\t\t\t\n",
    "# Set grid search params\n",
    "param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "param_range_fl = [1.0, 0.5, 0.1]\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "\t\t'clf__C': param_range_fl,\n",
    "\t\t'clf__solver': ['liblinear']}] \n",
    "\n",
    "grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "\t\t'clf__min_samples_leaf': param_range,\n",
    "\t\t'clf__max_depth': param_range,\n",
    "\t\t'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "grid_params_svm = [{'clf__kernel': ['linear', 'rbf'], \n",
    "\t\t'clf__C': param_range}]\n",
    "\n",
    "# Construct grid searches\n",
    "jobs = -1\n",
    "\n",
    "gs_lr = GridSearchCV(estimator=pipe_lr,\n",
    "\t\t\tparam_grid=grid_params_lr,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10) \n",
    "\t\t\t\n",
    "gs_lr_pca = GridSearchCV(estimator=pipe_lr_pca,\n",
    "\t\t\tparam_grid=grid_params_lr,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10)\n",
    "\t\t\t\n",
    "gs_rf = GridSearchCV(estimator=pipe_rf,\n",
    "\t\t\tparam_grid=grid_params_rf,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10, \n",
    "\t\t\tn_jobs=jobs)\n",
    "\n",
    "gs_rf_pca = GridSearchCV(estimator=pipe_rf_pca,\n",
    "\t\t\tparam_grid=grid_params_rf,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10, \n",
    "\t\t\tn_jobs=jobs)\n",
    "\n",
    "gs_svm = GridSearchCV(estimator=pipe_svm,\n",
    "\t\t\tparam_grid=grid_params_svm,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10,\n",
    "\t\t\tn_jobs=jobs)\n",
    "\n",
    "gs_svm_pca = GridSearchCV(estimator=pipe_svm_pca,\n",
    "\t\t\tparam_grid=grid_params_svm,\n",
    "\t\t\tscoring='accuracy',\n",
    "\t\t\tcv=10,\n",
    "\t\t\tn_jobs=jobs)\n",
    "\n",
    "# List of pipelines for ease of iteration\n",
    "grids = [gs_lr, gs_lr_pca, gs_rf, gs_rf_pca, gs_svm, gs_svm_pca]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_dict = {0: 'Logistic Regression', 1: 'Logistic Regression w/PCA', \n",
    "\t\t2: 'Random Forest', 3: 'Random Forest w/PCA', \n",
    "\t\t4: 'Support Vector Machine', 5: 'Support Vector Machine w/PCA'}\n",
    "\n",
    "# Fit the grid search objects\n",
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "for idx, gs in enumerate(grids):\n",
    "\tprint('\\nEstimator: %s' % grid_dict[idx])\t\n",
    "\t# Fit grid search\t\n",
    "\tgs.fit(X_train, y_train)\n",
    "\t# Best params\n",
    "\tprint('Best params: %s' % gs.best_params_)\n",
    "\t# Best training data accuracy\n",
    "\tprint('Best training accuracy: %.3f' % gs.best_score_)\n",
    "\t# Predict on test data with best params\n",
    "\ty_pred = gs.predict(X_test)\n",
    "\t# Test data accuracy of model with best params\n",
    "\tprint('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n",
    "\t# Track best (highest test accuracy) model\n",
    "\tif accuracy_score(y_test, y_pred) > best_acc:\n",
    "\t\tbest_acc = accuracy_score(y_test, y_pred)\n",
    "\t\tbest_gs = gs\n",
    "\t\tbest_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n",
    "\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results\n",
    "\n",
    "best model  on training is the SVM with a score of 0.967\n",
    "best on test data accuracy is random forest which got all classifications correct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML with TPOT\n",
    "\n",
    "[Using AutoML to Generate Machine Learning Pipelines with TPOT](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html)\n",
    "\n",
    "TPOT is a Python Automated Machine Learning tool that optimises machine learning pipelines using genetic programming.\n",
    "\n",
    "Data scientist and leading automated machine learning proponent [Randy Olson](http://www.randalolson.com/) states that [effective machine learning design requires us to](https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html/2):\n",
    "\n",
    "* Always tune the hyperparameters for our models\n",
    "* Always try out many different models\n",
    "* Always explore numerous feature representations for our data\n",
    "\n",
    "![Automated_TPOT](../../_images/2023-03-25-19-36-40.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceaaf6fd0944ffb81f1e1d41b499b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.9833333333333332\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.9916666666666668\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.9916666666666668\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(DecisionTreeClassifier(ExtraTreesClassifier(Normalizer(input_matrix, norm=l1), bootstrap=True, criterion=gini, max_features=0.7000000000000001, min_samples_leaf=4, min_samples_split=13, n_estimators=100), criterion=gini, max_depth=6, min_samples_leaf=2, min_samples_split=17), bootstrap=False, criterion=entropy, max_features=0.6500000000000001, min_samples_leaf=5, min_samples_split=13, n_estimators=100)\n",
      "TPOT classifier finished in 331.5765736103058 seconds\n",
      "Best pipeline test accuracy: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:794: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construct and fit TPOT classifier\n",
    "start_time = time.time()\n",
    "tpot = TPOTClassifier(generations=10, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Results\n",
    "print('TPOT classifier finished in %s seconds' % (end_time - start_time)) \n",
    "print('Best pipeline test accuracy: %.3f' % tpot.score(X_test, y_test))\n",
    "\n",
    "# Save best pipeline as Python script file\n",
    "tpot.export('tpot_iris_pipeline.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes\n",
    "\n",
    "Genetic programming-based hyperparameter selection, modelling with different algorithms and feature representations exploration.  \n",
    "\n",
    "\"Best\" model based on 'test set' accuracy displayed.\n",
    "\n",
    "## results\n",
    "\n",
    "* 5:40  to run\n",
    "* ExtraTreesClassifier pipeline accurately classified 100% of test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exported and saved as \n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: 0.9916666666666668\n",
    "exported_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"l1\"),\n",
    "    StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=True, criterion=\"gini\", max_features=0.7000000000000001, min_samples_leaf=4, min_samples_split=13, n_estimators=100)),\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"gini\", max_depth=6, min_samples_leaf=2, min_samples_split=17)),\n",
    "    ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.6500000000000001, min_samples_leaf=5, min_samples_split=13, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
