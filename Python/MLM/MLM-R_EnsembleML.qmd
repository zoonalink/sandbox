---
title: "MLM Ensemble ML Algorithms"
author: "p lovehagen"
format: html
---

## Details

<https://machinelearningmastery.com/machine-learning-ensembles-with-r/>

### Increase Accuracy

-   Shortlist of accurate modesl -\> algorithm tuning to improve each

OR

-   Increase accuracy by combining predictions of multiple different models


### Kappa

Kappa is a measure of the agreement between the predicted and actual classifications of a model, taking into account the possibility by chance.  Often used to evaluate model performance, especially when the classes are imbalanced.

Ranges from -1 to 1 - -1 is complete disagreement, 0 is agreement by chance, 1 is perfect agreement between predicted and actual classifications

Kappa is sensitive to prevalence of the classes.  It may be more appropriate to use other measures, like F1 score or the AUC-ROC.

Accuracy is measure of proportion correctly classified instances, not taking into account distribution of classes and potential imbalance.

### Methods

-   **Bagging**. Building multiple models (typically of the same type) from different subsamples of the training dataset.
-   **Boosting**. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.
-   **Stacking**. Building multiple models (typically of differing types) and supervisor model that learns how to best combine the predictions of the primary models.

### Test Dataset

Ionosphere dataset from [UCI ML Repo](https://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/)

```{r}
# Load libraries
library(mlbench)
library(caret)
library(caretEnsemble)

# Load the dataset
data(Ionosphere)
dataset <- Ionosphere
dataset <- dataset[,-2]
dataset$V1 <- as.numeric(as.character(dataset$V1))
```

```{r}
head(dataset)
```

## Boosting

-   C5.0
-   Stochastic Gradient Boosting

```{r}
# Example of Boosting Algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
# C5.0
set.seed(seed)
fit.c50 <- train(Class~., data=dataset, method="C5.0", metric=metric, trControl=control)
# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(Class~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
# summarize results
boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boosting_results)
dotplot(boosting_results)
```

gbm produces a more accurate model with an accuracy of 94.41

## Bagging

-   Bagged CART
-   Random Forest

```{r}
# Example of Bagging algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
# Bagged CART
set.seed(seed)
fit.treebag <- train(Class~., data=dataset, method="treebag", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.rf <- train(Class~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize results
bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(bagging_results)
dotplot(bagging_results)
```

Random forest has slightly higher accuracy 92.81%

```{r}
mean(bagging_results$values$`rf~Accuracy`)
```

## Stacking Algorithms

Can combine prediction of multiple **caret** modesl using `caretEnsemble` package.

Let's first look at creating 5 sub-models for the ionosphere dataset, specifically:

-   Linear Discriminate Analysis (LDA)
-   Classification and Regression Trees (CART)
-   Logistic Regression (via Generalized Linear Model or GLM)
-   k-Nearest Neighbors (kNN)
-   Support Vector Machine with a Radial Basis Kernel Function (SVM)


```{r}
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
models <- caretList(Class~., data=dataset, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
```
When combining models, we want the predictions of the submodels to have low correlation.  This means that they are skillful but in different ways to each other, so a new classifier will get the best out of each.

If the predictions for the sub-models are highly correlated (>0.75) they are making similar predictions most of the time, reducing the benefit of an ensemble.

```{r}
# correlation between results
modelCor(results)
splom(results)
```

Combine using a simple linear model

```{r}
# stack using glm
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)
```
Small improvement over single best algorithm (SVM - 94.8)


Random Forest method of combining

```{r}
# stack using random forest
set.seed(seed)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```

Impressive improvement over SVM alone - with 96.68%





