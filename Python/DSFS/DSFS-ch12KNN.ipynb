{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science from Scratch - Ch 12 - KNN\n",
    "\n",
    "Use kernel / env `dsfs` for this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors is one of the simplest predictive models there is. It makes no mathematical assumptions, and it doesn’t require any sort of heavy machinery. The only things it requires are:\n",
    "\n",
    "* Some notion of distance\n",
    "\n",
    "* An assumption that points that are close to one another are similar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify a new point by looking at its “nearest neighbors” in the training data. The new point is assigned the most common label among its k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def raw_majority_vote(labels: List[str]) -> str:\n",
    "    votes = Counter(labels)\n",
    "    winner, _ = votes.most_common(1)[0]\n",
    "    return winner\n",
    "\n",
    "assert raw_majority_vote(['a', 'b', 'c', 'b']) == 'b'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ties: \n",
    "\n",
    "* Pick one of the winners at random.\n",
    "* Weight the votes by distance and pick the weighted winner.\n",
    "* Reduce k until we find a unique winner (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(labels: List[str]) -> str:\n",
    "    \"\"\"Assumes that labels are ordered from nearest to farthest.\"\"\"\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count\n",
    "                       for count in vote_counts.values()\n",
    "                       if count == winner_count])\n",
    "\n",
    "    if num_winners == 1:\n",
    "        return winner                     # unique winner, so return it\n",
    "    else:\n",
    "        return majority_vote(labels[:-1]) # try again without the farthest\n",
    "\n",
    "# Tie, so look at first 4, then 'b'\n",
    "assert majority_vote(['a', 'b', 'c', 'b', 'a']) == 'b'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from scratch.linear_algebra import Vector, distance\n",
    "\n",
    "\n",
    "class LabeledPoint(NamedTuple):\n",
    "    point: Vector\n",
    "    label: str\n",
    "\n",
    "def knn_classify(k: int,\n",
    "                 labeled_points: List[LabeledPoint],\n",
    "                 new_point: Vector) -> str:\n",
    "\n",
    "    # Order the labeled points from nearest to farthest.\n",
    "    by_distance = sorted(labeled_points,\n",
    "                         key=lambda lp: distance(lp.point, new_point))\n",
    "\n",
    "    # Find the labels for the k closest\n",
    "    k_nearest_labels = [lp.label for lp in by_distance[:k]]\n",
    "\n",
    "    # and let them vote.\n",
    "    return majority_vote(k_nearest_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get(\n",
    "  \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    ")\n",
    "\n",
    "with open('iris.dat', 'w') as f:\n",
    "    f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_iris_row(row: List[str]) -> LabeledPoint:\n",
    "    \"\"\"\n",
    "    sepal_length, sepal_width, petal_length, petal_width, class\n",
    "    \"\"\"\n",
    "    measurements = [float(value) for value in row[:-1]]\n",
    "    # class is e.g. \"Iris-virginica\"; we just want \"virginica\"\n",
    "    label = row[-1].split(\"-\")[-1]\n",
    "\n",
    "    return LabeledPoint(measurements, label)\n",
    "\n",
    "with open('iris.data') as f:\n",
    "    reader = csv.reader(f)\n",
    "    iris_data = [parse_iris_row(row) for row in reader]\n",
    "\n",
    "# We'll also group just the points by species/label so we can plot them\n",
    "points_by_species: Dict[str, List[Vector]] = defaultdict(list)\n",
    "for iris in iris_data:\n",
    "    points_by_species[iris.label].append(iris.point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "metrics = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "pairs = [(i, j) for i in range(4) for j in range(4) if i < j]\n",
    "marks = ['+', '.', 'x']  # we have 3 classes, so 3 markers\n",
    "\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        i, j = pairs[3 * row + col]\n",
    "        ax[row][col].set_title(f\"{metrics[i]} vs {metrics[j]}\", fontsize=8)\n",
    "        ax[row][col].set_xticks([])\n",
    "        ax[row][col].set_yticks([])\n",
    "\n",
    "        for mark, (species, points) in zip(marks, points_by_species.items()):\n",
    "            xs = [point[i] for point in points]\n",
    "            ys = [point[j] for point in points]\n",
    "            ax[row][col].scatter(xs, ys, marker=mark, label=species)\n",
    "\n",
    "ax[-1][-1].legend(loc='lower right', prop={'size': 6})\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.machine_learning import split_data\n",
    "\n",
    "random.seed(12)\n",
    "iris_train, iris_test = split_data(iris_data, 0.70)\n",
    "assert len(iris_train) == 0.7 * 150\n",
    "assert len(iris_test) == 0.3 * 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# track how many times we see (predicted, actual)\n",
    "confusion_matrix: Dict[Tuple[str, str], int] = defaultdict(int)\n",
    "num_correct = 0\n",
    "\n",
    "for iris in iris_test:\n",
    "    predicted = knn_classify(5, iris_train, iris.point)\n",
    "    actual = iris.label\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "\n",
    "    confusion_matrix[(predicted, actual)] += 1\n",
    "\n",
    "pct_correct = num_correct / len(iris_test)\n",
    "print(pct_correct, confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## curse of dimensionality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_point(dim: int) -> Vector:\n",
    "    return [random.random() for _ in range(dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_distances(dim: int, num_pairs: int) -> List[float]:\n",
    "    return [distance(random_point(dim), random_point(dim))\n",
    "            for _ in range(num_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "dimensions = range(1, 101)\n",
    "\n",
    "avg_distances = []\n",
    "min_distances = []\n",
    "\n",
    "random.seed(0)\n",
    "for dim in tqdm.tqdm(dimensions, desc=\"Curse of Dimensionality\"):\n",
    "    distances = random_distances(dim, 10000)      # 10,000 random pairs\n",
    "    avg_distances.append(sum(distances) / 10000)  # track the average\n",
    "    min_distances.append(min(distances))          # track the minimum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the number of dimensions increases, the average distance between poitns increases; the ratio between closest distance and averge distance increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_avg_ratio = [min_dist / avg_dist\n",
    "                 for min_dist, avg_dist in zip(min_distances, avg_distances)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
