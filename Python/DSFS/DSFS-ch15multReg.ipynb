{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science from Scratch - ch 15 - Multiple Regression\n",
    "\n",
    "use kernel / env : `dsfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from typing import List\n",
    "\n",
    "inputs: List[List[float]] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4]  # so prediction = 4 + 8 + 12 = 24\n",
    "\n",
    "assert error(x, y, beta) == -6\n",
    "assert squared_error(x, y, beta) == 36\n",
    "\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x, y, beta) == [-12, -24, -36]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')        # Generic type for data\n",
    "Stat = TypeVar('Stat')  # Generic type for \"statistic\"\n",
    "\n",
    "def bootstrap_sample(data: List[X]) -> List[X]:\n",
    "    \"\"\"randomly samples len(data) elements with replacement\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_statistic(data: List[X],\n",
    "                        stats_fn: Callable[[List[X]], Stat],\n",
    "                        num_samples: int) -> List[Stat]:\n",
    "    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]\n",
    "\n",
    "# 101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 points, 50 of them near 0, 50 of them near 200\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import median, standard_deviation\n",
    "\n",
    "medians_close = bootstrap_statistic(close_to_100, median, 100)\n",
    "\n",
    "medians_far = bootstrap_statistic(far_from_100, median, 100)\n",
    "\n",
    "assert standard_deviation(medians_close) < 1\n",
    "assert standard_deviation(medians_far) > 90\n",
    "\n",
    "from scratch.probability import normal_cdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(beta_hat_j: float, sigma_hat_j: float) -> float:\n",
    "    if beta_hat_j > 0:\n",
    "        # if the coefficient is positive, we need to compute twice the\n",
    "        # probability of seeing an even *larger* value\n",
    "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        # otherwise twice the probability of seeing a *smaller* value\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "\n",
    "assert p_value(30.58, 1.27)   < 0.001  # constant term\n",
    "assert p_value(0.972, 0.103)  < 0.001  # num_friends\n",
    "assert p_value(-1.865, 0.155) < 0.001  # work_hours\n",
    "assert p_value(0.923, 1.249)  > 0.4    # phd\n",
    "\n",
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "# sometimes it's called \"lambda\" but that already means something in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_penalty(beta: Vector, alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_ridge(x: Vector,\n",
    "                        y: float,\n",
    "                        beta: Vector,\n",
    "                        alpha: float) -> float:\n",
    "    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "from scratch.linear_algebra import add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_penalty_gradient(beta: Vector, alpha: float) -> Vector:\n",
    "    \"\"\"gradient of just the ridge penalty\"\"\"\n",
    "    return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_ridge_gradient(x: Vector,\n",
    "                           y: float,\n",
    "                           beta: Vector,\n",
    "                           alpha: float) -> Vector:\n",
    "    \"\"\"\n",
    "    the gradient corresponding to the ith squared error term\n",
    "    including the ridge penalty\n",
    "    \"\"\"\n",
    "    return add(sqerror_gradient(x, y, beta),\n",
    "               ridge_penalty_gradient(beta, alpha))\n",
    "\n",
    "\n",
    "from scratch.statistics import daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit_ridge(xs: List[Vector],\n",
    "                            ys: List[float],\n",
    "                            alpha: float,\n",
    "                            learning_rate: float,\n",
    "                            num_steps: int,\n",
    "                            batch_size: int = 1) -> Vector:\n",
    "    # Start guess with mean\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_ridge_gradient(x, y, guess, alpha)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from scratch.statistics import daily_minutes_good\n",
    "    from scratch.gradient_descent import gradient_step\n",
    "    \n",
    "    random.seed(0)\n",
    "    # I used trial and error to choose niters and step_size.\n",
    "    # This will run for a while.\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    beta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25)\n",
    "    assert 30.50 < beta[0] < 30.70  # constant\n",
    "    assert  0.96 < beta[1] <  1.00  # num friends\n",
    "    assert -1.89 < beta[2] < -1.85  # work hours per day\n",
    "    assert  0.91 < beta[3] <  0.94  # has PhD\n",
    "    \n",
    "    assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68\n",
    "    \n",
    "    from typing import Tuple\n",
    "    \n",
    "    import datetime\n",
    "    \n",
    "    def estimate_sample_beta(pairs: List[Tuple[Vector, float]]):\n",
    "        x_sample = [x for x, _ in pairs]\n",
    "        y_sample = [y for _, y in pairs]\n",
    "        beta = least_squares_fit(x_sample, y_sample, learning_rate, 5000, 25)\n",
    "        print(\"bootstrap sample\", beta)\n",
    "        return beta\n",
    "    \n",
    "    random.seed(0) # so that you get the same results as me\n",
    "    \n",
    "    # This will take a couple of minutes!\n",
    "    bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)),\n",
    "                                          estimate_sample_beta,\n",
    "                                          100)\n",
    "    \n",
    "    bootstrap_standard_errors = [\n",
    "        standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "        for i in range(4)]\n",
    "    \n",
    "    print(bootstrap_standard_errors)\n",
    "    \n",
    "    # [1.272,    # constant term, actual error = 1.19\n",
    "    #  0.103,    # num_friends,   actual error = 0.080\n",
    "    #  0.155,    # work_hours,    actual error = 0.127\n",
    "    #  1.249]    # phd,           actual error = 0.998\n",
    "    \n",
    "    random.seed(0)\n",
    "    beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0,  # alpha\n",
    "                                     learning_rate, 5000, 25)\n",
    "    # [30.51, 0.97, -1.85, 0.91]\n",
    "    assert 5 < dot(beta_0[1:], beta_0[1:]) < 6\n",
    "    assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0) < 0.69\n",
    "    \n",
    "    beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1,  # alpha\n",
    "                                       learning_rate, 5000, 25)\n",
    "    # [30.8, 0.95, -1.83, 0.54]\n",
    "    assert 4 < dot(beta_0_1[1:], beta_0_1[1:]) < 5\n",
    "    assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0_1) < 0.69\n",
    "    \n",
    "    \n",
    "    beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1,  # alpha\n",
    "                                     learning_rate, 5000, 25)\n",
    "    # [30.6, 0.90, -1.68, 0.10]\n",
    "    assert 3 < dot(beta_1[1:], beta_1[1:]) < 4\n",
    "    assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_1) < 0.69\n",
    "    \n",
    "    beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good,10,  # alpha\n",
    "                                      learning_rate, 5000, 25)\n",
    "    # [28.3, 0.67, -0.90, -0.01]\n",
    "    assert 1 < dot(beta_10[1:], beta_10[1:]) < 2\n",
    "    assert 0.5 < multiple_r_squared(inputs, daily_minutes_good, beta_10) < 0.6\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is an approach in which we add to the error term a penalty that gets larger as beta gets larger. We then minimize the combined error and penalty. The more importance we place on the penalty term, the more we discourage large coefficients.\n",
    "\n",
    "For example, in ridge regression, we add a penalty proportional to the sum of the squares of the beta_i (except that typically we don’t penalize beta_0, the constant term)\n",
    "\n",
    "Whereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to force coefficients to be 0, which makes it good for learning sparse models. Unfortunately, it’s not amenable to gradient descent, which means that we won’t be able to solve it from scratch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge and Lasso regularization are techniques used in linear regression to help prevent overfitting of the model. Overfitting occurs when a model is too complex and fits the training data too closely, but does not generalize well to new data.\n",
    "\n",
    "Both Ridge and Lasso regularization work by adding a penalty term to the standard linear regression objective function, which helps to limit the size of the coefficients of the regression equation. This, in turn, helps to simplify the model and reduce overfitting.\n",
    "\n",
    "Ridge regularization adds a penalty term to the sum of squared coefficients (also known as the L2 norm) of the regression equation. The penalty term is proportional to the square of the magnitude of the coefficients, so it shrinks them towards zero but doesn't set them to exactly zero. This means that Ridge regularization tends to preserve all features in the model, even if they are not very important.\n",
    "\n",
    "Lasso regularization, on the other hand, adds a penalty term to the sum of absolute values of the coefficients (also known as the L1 norm) of the regression equation. Unlike Ridge regularization, Lasso regularization can set some of the coefficients to exactly zero, effectively removing them from the model. This means that Lasso regularization tends to produce sparse models that only include the most important features.\n",
    "\n",
    "In summary, Ridge and Lasso regularization are techniques used in linear regression to prevent overfitting by adding a penalty term to the objective function that limits the size of the coefficients of the regression equation. Ridge regularization tends to preserve all features in the model, while Lasso regularization tends to produce sparse models that only include the most important features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
