{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost in pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* takes list of named 2-tuples as input (name, pipeline step)\n",
    "* tuples can contain any arbitrary scikit-learn compatible estimator or transformer\n",
    "* pipeline implements fit, predict, score and other methods\n",
    "* can be used as input estimator for GridSearchCV and other scikit-learn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores (neg_mean_squared_error): [-0.51763375 -0.34609122 -0.37323673 -0.44590967 -0.47167993]\n",
      "Average RMSE: 0.6546496345930508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load data\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Evaluate pipeline\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Output scores\n",
    "print(\"Cross-validated scores (neg_mean_squared_error):\", scores)\n",
    "\n",
    "# print final average rmse - by taking the square root of the absolute negative mean squared error\n",
    "print(\"Average RMSE:\", np.mean(np.sqrt(np.abs(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "### preprocessing 1: LabelEncoder and OneHotEncoder\n",
    "* LabelEncoder: converts string labels to integers\n",
    "* OneHotEncoder: converts integer labels to one-hot vectors\n",
    "\n",
    "-> cannot be done with a pipeline\n",
    "\n",
    "### Preprocessing 2: DictVectorizer\n",
    "\n",
    "* DictVectorizer: converts lists of feature mappings to vectors\n",
    "* convert DataFrame into list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ames data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "ames = pd.read_csv('ames_unprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass MSZoning  LotFrontage  LotArea Neighborhood BldgType HouseStyle  \\\n",
      "0          60       RL         65.0     8450      CollgCr     1Fam     2Story   \n",
      "1          20       RL         80.0     9600      Veenker     1Fam     1Story   \n",
      "2          60       RL         68.0    11250      CollgCr     1Fam     2Story   \n",
      "3          70       RL         60.0     9550      Crawfor     1Fam     2Story   \n",
      "4          60       RL         84.0    14260      NoRidge     1Fam     2Story   \n",
      "\n",
      "   OverallQual  OverallCond  YearBuilt  ...  GrLivArea  BsmtFullBath  \\\n",
      "0            7            5       2003  ...       1710             1   \n",
      "1            6            8       1976  ...       1262             0   \n",
      "2            7            5       2001  ...       1786             1   \n",
      "3            7            5       1915  ...       1717             1   \n",
      "4            8            5       2000  ...       2198             1   \n",
      "\n",
      "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  GarageArea  \\\n",
      "0             0         2         1             3           0         548   \n",
      "1             1         2         0             3           1         460   \n",
      "2             0         2         1             3           1         608   \n",
      "3             0         1         0             3           1         642   \n",
      "4             0         2         1             4           1         836   \n",
      "\n",
      "   PavedDrive SalePrice  \n",
      "0           Y    208500  \n",
      "1           Y    181500  \n",
      "2           Y    223500  \n",
      "3           Y    140000  \n",
      "4           Y    250000  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 21 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   MSSubClass    1460 non-null   int64  \n",
      " 1   MSZoning      1460 non-null   object \n",
      " 2   LotFrontage   1201 non-null   float64\n",
      " 3   LotArea       1460 non-null   int64  \n",
      " 4   Neighborhood  1460 non-null   object \n",
      " 5   BldgType      1460 non-null   object \n",
      " 6   HouseStyle    1460 non-null   object \n",
      " 7   OverallQual   1460 non-null   int64  \n",
      " 8   OverallCond   1460 non-null   int64  \n",
      " 9   YearBuilt     1460 non-null   int64  \n",
      " 10  Remodeled     1460 non-null   int64  \n",
      " 11  GrLivArea     1460 non-null   int64  \n",
      " 12  BsmtFullBath  1460 non-null   int64  \n",
      " 13  BsmtHalfBath  1460 non-null   int64  \n",
      " 14  FullBath      1460 non-null   int64  \n",
      " 15  HalfBath      1460 non-null   int64  \n",
      " 16  BedroomAbvGr  1460 non-null   int64  \n",
      " 17  Fireplaces    1460 non-null   int64  \n",
      " 18  GarageArea    1460 non-null   int64  \n",
      " 19  PavedDrive    1460 non-null   object \n",
      " 20  SalePrice     1460 non-null   int64  \n",
      "dtypes: float64(1), int64(15), object(5)\n",
      "memory usage: 239.7+ KB\n",
      "None\n",
      "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
      "count  1460.000000  1201.000000    1460.000000  1460.000000  1460.000000   \n",
      "mean     56.897260    70.049958   10516.828082     6.099315     5.575342   \n",
      "std      42.300571    24.284752    9981.264932     1.382997     1.112799   \n",
      "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
      "25%      20.000000    59.000000    7553.500000     5.000000     5.000000   \n",
      "50%      50.000000    69.000000    9478.500000     6.000000     5.000000   \n",
      "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
      "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
      "\n",
      "         YearBuilt    Remodeled    GrLivArea  BsmtFullBath  BsmtHalfBath  \\\n",
      "count  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000   \n",
      "mean   1971.267808     0.476712  1515.463699      0.425342      0.057534   \n",
      "std      30.202904     0.499629   525.480383      0.518911      0.238753   \n",
      "min    1872.000000     0.000000   334.000000      0.000000      0.000000   \n",
      "25%    1954.000000     0.000000  1129.500000      0.000000      0.000000   \n",
      "50%    1973.000000     0.000000  1464.000000      0.000000      0.000000   \n",
      "75%    2000.000000     1.000000  1776.750000      1.000000      0.000000   \n",
      "max    2010.000000     1.000000  5642.000000      3.000000      2.000000   \n",
      "\n",
      "          FullBath     HalfBath  BedroomAbvGr   Fireplaces   GarageArea  \\\n",
      "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
      "mean      1.565068     0.382877      2.866438     0.613014   472.980137   \n",
      "std       0.550916     0.502885      0.815778     0.644666   213.804841   \n",
      "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
      "25%       1.000000     0.000000      2.000000     0.000000   334.500000   \n",
      "50%       2.000000     0.000000      3.000000     1.000000   480.000000   \n",
      "75%       2.000000     1.000000      3.000000     1.000000   576.000000   \n",
      "max       3.000000     2.000000      8.000000     3.000000  1418.000000   \n",
      "\n",
      "           SalePrice  \n",
      "count    1460.000000  \n",
      "mean   180921.195890  \n",
      "std     79442.502883  \n",
      "min     34900.000000  \n",
      "25%    129975.000000  \n",
      "50%    163000.000000  \n",
      "75%    214000.000000  \n",
      "max    755000.000000  \n"
     ]
    }
   ],
   "source": [
    "# print the first few rows of the data\n",
    "print(ames.head())\n",
    "\n",
    "# print info \n",
    "print(ames.info())\n",
    "\n",
    "# print summary statistics\n",
    "print(ames.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "ames['LotFrontage'] = ames['LotFrontage'].fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (ames.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = ames.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(ames[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "ames[categorical_columns] = ames[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(ames[categorical_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to 'one-hot-encode' the categorical variables so that they can be used in the model.  If this is not done then the model will treat the categorical variables as continuous and will not be able to use them effectively.  The model will assume an order to the categories which is not there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(1460, 21)\n",
      "(1460, 3369)\n"
     ]
    }
   ],
   "source": [
    "# import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(ames)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(ames.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simplify LabelEncoder and OneHotEncoder with DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
      "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
      "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
      "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
      "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
      "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
      "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
      "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
      "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
     ]
    }
   ],
   "source": [
    "# import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# convert ames to a dictionary\n",
    "ames_dict = ames.to_dict(orient='records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on ames_dict\n",
    "df_encoded = dv.fit_transform(ames_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocssing within a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;ohe_onestep&#x27;, DictVectorizer(sparse=False)),\n",
       "                (&#x27;xgb_model&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;ohe_onestep&#x27;, DictVectorizer(sparse=False)),\n",
       "                (&#x27;xgb_model&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(sparse=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('ohe_onestep', DictVectorizer(sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import DictVectorizer and Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "ames['LotFrontage'] = ames['LotFrontage'].fillna(0)\n",
    "\n",
    "# set up pipeline steps\n",
    "steps = [\n",
    "    ('ohe_onestep', DictVectorizer(sparse=False)),\n",
    "    ('xgb_model', XGBRegressor())\n",
    "]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# fit the pipeline\n",
    "xgb_pipeline.fit(ames.to_dict(orient='records'), ames['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost in pipeline\n",
    "\n",
    "* sklearn_pandas.DataFrameMapper: applies sklearn-compatible transformers to columns of a pandas DataFrame\n",
    "* sklearn.impute.SimpleImputer: impute missing values\n",
    "* sklearn.pipeline.FeatureUnion: combine multiple pipelines into a single pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  3349.964400032365\n"
     ]
    }
   ],
   "source": [
    "#import dictvectorizer, pipelien,cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "ames['LotFrontage'] = ames['LotFrontage'].fillna(0)\n",
    "\n",
    "# set up pipeline steps\n",
    "steps = [\n",
    "    ('ohe_onestep', DictVectorizer(sparse=False)),\n",
    "    ('xgb_model', XGBRegressor(max_depth=2, objective='reg:squarederror'))\n",
    "]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, ames.to_dict(orient='records'), ames['SalePrice'], scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chronic kidney disease data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              400 non-null    int64  \n",
      " 1   age             391 non-null    float64\n",
      " 2   bp              388 non-null    float64\n",
      " 3   sg              353 non-null    float64\n",
      " 4   al              354 non-null    float64\n",
      " 5   su              351 non-null    float64\n",
      " 6   rbc             248 non-null    object \n",
      " 7   pc              335 non-null    object \n",
      " 8   pcc             396 non-null    object \n",
      " 9   ba              396 non-null    object \n",
      " 10  bgr             356 non-null    float64\n",
      " 11  bu              381 non-null    float64\n",
      " 12  sc              383 non-null    float64\n",
      " 13  sod             313 non-null    float64\n",
      " 14  pot             312 non-null    float64\n",
      " 15  hemo            348 non-null    float64\n",
      " 16  pcv             330 non-null    object \n",
      " 17  wc              295 non-null    object \n",
      " 18  rc              270 non-null    object \n",
      " 19  htn             398 non-null    object \n",
      " 20  dm              398 non-null    object \n",
      " 21  cad             398 non-null    object \n",
      " 22  appet           399 non-null    object \n",
      " 23  pe              399 non-null    object \n",
      " 24  ane             399 non-null    object \n",
      " 25  classification  400 non-null    object \n",
      "dtypes: float64(11), int64(1), object(14)\n",
      "memory usage: 81.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "ckd = pd.read_csv('ChronicKidneyDisease.csv')\n",
    "\n",
    "# trim white space and /t from the ckd['classification'] column\n",
    "ckd['classification'] = ckd['classification'].str.strip()\n",
    "\n",
    "print(ckd.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age        9\n",
      "bp        12\n",
      "sg        47\n",
      "al        46\n",
      "su        49\n",
      "rbc      152\n",
      "pc        65\n",
      "pcc        4\n",
      "ba         4\n",
      "bgr       44\n",
      "bu        19\n",
      "sc        17\n",
      "sod       87\n",
      "pot       88\n",
      "hemo      52\n",
      "pcv       70\n",
      "wc       105\n",
      "rc       130\n",
      "htn        2\n",
      "dm         2\n",
      "cad        2\n",
      "appet      1\n",
      "pe         1\n",
      "ane        1\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# import dataframemapper, simpleimputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X, y = ckd.drop(['id','classification'], axis=1), ckd['classification']\n",
    "\n",
    "# print unique values of ckd\n",
    "#print(y.unique())\n",
    "\n",
    "\n",
    "# change ckd to 0/1\n",
    "y = y.map({'ckd': 1, 'notckd': 0})\n",
    "\n",
    "# check nulls for X\n",
    "print(X.isnull().sum())\n",
    "\n",
    "print(y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckd       250\n",
      "notckd    150\n",
      "Name: classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print vlues of classification in ckd\n",
    "print(ckd['classification'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (X.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_mask].tolist()\n",
    "\n",
    "# get list of non-categorical columns\n",
    "non_categorical_columns = X.columns[~categorical_mask].tolist()\n",
    "\n",
    "# apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [(numeric_feature, SimpleImputer(strategy='median')) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, SimpleImputer(strategy='most_frequent')) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combiine numerical and categorical preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          ('num_mapper', numeric_imputation_mapper),\n",
    "                                          ('cat_mapper', categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo']\n",
      "['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n"
     ]
    }
   ],
   "source": [
    "print(non_categorical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined transformed:\n",
      "     age    bp     sg   al   su    bgr    bu   sc    sod  pot  ...          ba  \\\n",
      "0  48.0  80.0   1.02  1.0  0.0  121.0  36.0  1.2  138.0  4.4  ...  notpresent   \n",
      "1   7.0  50.0   1.02  4.0  0.0  121.0  18.0  0.8  138.0  4.4  ...  notpresent   \n",
      "2  62.0  80.0   1.01  2.0  3.0  423.0  53.0  1.8  138.0  4.4  ...  notpresent   \n",
      "3  48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.5  ...  notpresent   \n",
      "4  51.0  80.0   1.01  2.0  0.0  106.0  26.0  1.4  138.0  4.4  ...  notpresent   \n",
      "\n",
      "  pcv    wc   rc  htn   dm cad appet   pe  ane  \n",
      "0  44  7800  5.2  yes  yes  no  good   no   no  \n",
      "1  38  6000  5.2   no   no  no  good   no   no  \n",
      "2  31  7500  5.2   no  yes  no  poor   no  yes  \n",
      "3  32  6700  3.9  yes   no  no  poor  yes  yes  \n",
      "4  35  7300  4.6   no   no  no  good   no   no  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "3-fold AUC:  0.9987177280550773\n",
      "Best parameters found:  {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__n_estimators': 100}\n",
      "Best cross-validation AUC:  0.9991193344807803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to reshape columns\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.values.reshape(-1, 1)\n",
    "\n",
    "# Ensure X is a DataFrame\n",
    "if not isinstance(X, pd.DataFrame):\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "numeric_columns = non_categorical_columns  # Assuming non_categorical_columns is already defined\n",
    "categorical_columns = categorical_columns  # Assuming categorical_columns is already defined\n",
    "\n",
    "# Apply numeric imputer with reshaping\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [(numeric_feature, [ReshapeTransformer(), SimpleImputer(strategy='median')]) for numeric_feature in numeric_columns],\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    "    default=None\n",
    ")\n",
    "\n",
    "# Apply categorical imputer with reshaping\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, [ReshapeTransformer(), SimpleImputer(strategy='most_frequent')]) for category_feature in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    "    default=None\n",
    ")\n",
    "\n",
    "# Combine the numeric and categorical transformations using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_imputation_mapper, numeric_columns),\n",
    "        ('cat', categorical_imputation_mapper, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a transformer to convert the output of ColumnTransformer back to DataFrame\n",
    "class DataFrameTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.columns)\n",
    "\n",
    "# Create dictifier\n",
    "class Dictifier(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.to_dict(orient='records')\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_dataframe', DataFrameTransformer(columns=numeric_columns + categorical_columns)),\n",
    "    ('dictifier', Dictifier()),\n",
    "    ('vectorizer', DictVectorizer(sort=False)),\n",
    "    ('clf', XGBClassifier(max_depth=3))\n",
    "])\n",
    "\n",
    "# Check the output of each step\n",
    "# Step 1: Combined numeric and categorical transformations\n",
    "combined_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Manually specify the column names for the combined DataFrame\n",
    "combined_columns = numeric_columns + categorical_columns\n",
    "combined_transformed_df = pd.DataFrame(combined_transformed, columns=combined_columns)\n",
    "print(\"Combined transformed:\\n\", combined_transformed_df.head())\n",
    "\n",
    "# Step 2: Full pipeline\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=3, error_score='raise')\n",
    "\n",
    "# Print the 3-fold AUC scores\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "\n",
    "\n",
    "# Optional: Hyperparameter tuning using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth': [3, 5, 7],\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Numeric and Categorical Columns**: Ensure `numeric_columns` and `categorical_columns` are defined.\n",
    "2. **Numeric Imputer**: Apply the numeric imputation only to `numeric_columns`.\n",
    "3. **Categorical Imputer**: Apply the categorical imputation only to `categorical_columns`.\n",
    "4. **Combined Transformations**: Use `ColumnTransformer` to combine numeric and categorical transformations.\n",
    "5. **DataFrameTransformer**: Convert the output of `ColumnTransformer` back to a DataFrame.\n",
    "6. **Pipeline**: Create a pipeline that includes the combined transformations, conversion to DataFrame, dictifier, vectorizer, and classifier.\n",
    "7. **Check Output**: Print the combined transformed DataFrame to verify the transformations.\n",
    "8. **Fit Pipeline**: Fit the pipeline to the data.\n",
    "9. **Cross-Validation**: Perform cross-validation on the pipeline and print the 3-fold AUC scores.\n",
    "\n",
    "This should ensure that the pipeline is correctly defined, fitted, and evaluated using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's the updated pipeline that includes `StandardScaler` and uses `RandomizedSearchCV` with the specified parameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined transformed:\n",
      "     age    bp     sg   al   su    bgr    bu   sc    sod  pot  ...          ba  \\\n",
      "0  48.0  80.0   1.02  1.0  0.0  121.0  36.0  1.2  138.0  4.4  ...  notpresent   \n",
      "1   7.0  50.0   1.02  4.0  0.0  121.0  18.0  0.8  138.0  4.4  ...  notpresent   \n",
      "2  62.0  80.0   1.01  2.0  3.0  423.0  53.0  1.8  138.0  4.4  ...  notpresent   \n",
      "3  48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.5  ...  notpresent   \n",
      "4  51.0  80.0   1.01  2.0  0.0  106.0  26.0  1.4  138.0  4.4  ...  notpresent   \n",
      "\n",
      "  pcv    wc   rc  htn   dm cad appet   pe  ane  \n",
      "0  44  7800  5.2  yes  yes  no  good   no   no  \n",
      "1  38  6000  5.2   no   no  no  good   no   no  \n",
      "2  31  7500  5.2   no  yes  no  poor   no  yes  \n",
      "3  32  6700  3.9  yes   no  no  poor  yes  yes  \n",
      "4  35  7300  4.6   no   no  no  good   no   no  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "3-fold AUC:  0.9987177280550773\n",
      "Best parameters found:  {'clf__subsample': 0.5, 'clf__max_depth': 12, 'clf__colsample_bytree': 0.1}\n",
      "Best cross-validation AUC:  0.9997854997854998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to reshape columns\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.values.reshape(-1, 1)\n",
    "\n",
    "# Ensure X is a DataFrame\n",
    "if not isinstance(X, pd.DataFrame):\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "numeric_columns = non_categorical_columns  # Assuming non_categorical_columns is already defined\n",
    "categorical_columns = categorical_columns  # Assuming categorical_columns is already defined\n",
    "\n",
    "# Apply numeric imputer with reshaping\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [(numeric_feature, [ReshapeTransformer(), SimpleImputer(strategy='median')]) for numeric_feature in numeric_columns],\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    "    default=None\n",
    ")\n",
    "\n",
    "# Apply categorical imputer with reshaping\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, [ReshapeTransformer(), SimpleImputer(strategy='most_frequent')]) for category_feature in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    "    default=None\n",
    ")\n",
    "\n",
    "# Combine the numeric and categorical transformations using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_imputation_mapper, numeric_columns),\n",
    "        ('cat', categorical_imputation_mapper, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a transformer to convert the output of ColumnTransformer back to DataFrame\n",
    "class DataFrameTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.columns)\n",
    "\n",
    "# Create dictifier\n",
    "class Dictifier(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.to_dict(orient='records')\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_dataframe', DataFrameTransformer(columns=numeric_columns + categorical_columns)),\n",
    "    ('dictifier', Dictifier()),\n",
    "    ('vectorizer', DictVectorizer(sort=False)),\n",
    "    ('clf', XGBClassifier(max_depth=3))\n",
    "])\n",
    "\n",
    "# Check the output of each step\n",
    "# Step 1: Combined numeric and categorical transformations\n",
    "combined_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Manually specify the column names for the combined DataFrame\n",
    "combined_columns = numeric_columns + categorical_columns\n",
    "combined_transformed_df = pd.DataFrame(combined_transformed, columns=combined_columns)\n",
    "print(\"Combined transformed:\\n\", combined_transformed_df.head())\n",
    "\n",
    "# Step 2: Full pipeline\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=3, error_score='raise')\n",
    "\n",
    "# Print the 3-fold AUC scores\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'clf__subsample': np.arange(0.5, 1.0, 0.5),\n",
    "    'clf__max_depth': np.arange(3, 20, 1),\n",
    "    'clf__colsample_bytree': np.arange(0.1, 1.05, 0.05)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring='roc_auc',\n",
    "    cv=4,\n",
    "    n_iter=10,\n",
    "    random_state=42,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation AUC: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.9993584914557583\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  DataFrameMapper(default=None,\n",
      "                                                                  df_out=True,\n",
      "                                                                  drop_cols=[],\n",
      "                                                                  features=[('age',\n",
      "                                                                             [ReshapeTransformer(),\n",
      "                                                                              SimpleImputer(strategy='median')]),\n",
      "                                                                            ('bp',\n",
      "                                                                             [ReshapeTransformer(),\n",
      "                                                                              SimpleImputer(strategy='median')]),\n",
      "                                                                            ('sg',\n",
      "                                                                             [ReshapeTransformer(),\n",
      "                                                                              SimpleImputer(strategy='median')]),\n",
      "                                                                            ('al',\n",
      "                                                                             [ReshapeTransformer...\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=12, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=100,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "# print rmse \n",
    "print(\"RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
    "\n",
    "# print the best model\n",
    "print(random_search.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Numeric and Categorical Columns**: Ensure \n",
    "\n",
    "numeric_columns\n",
    "\n",
    " and \n",
    "\n",
    "categorical_columns\n",
    "\n",
    " are defined.\n",
    "2. **Numeric Imputer and Scaler**: Apply the numeric imputation and scaling only to \n",
    "\n",
    "numeric_columns\n",
    "\n",
    ".\n",
    "3. **Categorical Imputer**: Apply the categorical imputation only to \n",
    "\n",
    "categorical_columns\n",
    "\n",
    ".\n",
    "4. **Combined Transformations**: Use `ColumnTransformer` to combine numeric and categorical transformations.\n",
    "5. **Pipeline**: Create a pipeline that includes the combined transformations, vectorizer, and classifier.\n",
    "6. **Parameter Grid**: Define the parameter grid for `RandomizedSearchCV` with the specified ranges.\n",
    "7. **RandomizedSearchCV**: Create and fit `RandomizedSearchCV` with the pipeline and parameter grid.\n",
    "8. **Print Results**: Print the best parameters and best cross-validation AUC score.\n",
    "\n",
    "This version of the pipeline includes `StandardScaler` for numeric features and uses `RandomizedSearchCV` to find the best hyperparameters for the `XGBClassifier`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
