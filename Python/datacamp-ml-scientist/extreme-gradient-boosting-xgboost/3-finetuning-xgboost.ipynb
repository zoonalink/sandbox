{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned rmse: 33288.916054\n"
     ]
    }
   ],
   "source": [
    "# untuned model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "housing_data = pd.read_csv('ames_housing_trimmed_processed.csv')\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "untuned_params = {\"objective\":\"reg:squarederror\"}\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, \n",
    "num_boost_round=200, \n",
    "metrics=\"rmse\", as_pandas=True, seed = 123)\n",
    "print(\"Untuned rmse: %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 29965.411196\n"
     ]
    }
   ],
   "source": [
    "# TUNED MODEL\n",
    "tuned_params = {\"objective\":\"reg:squarederror\", \"colsample_bytree\":0.3, \"learning_rate\":0.1, \"max_depth\":5}\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, \n",
    "num_boost_round=200, \n",
    "metrics=\"rmse\", as_pandas=True, seed = 123)\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tuning number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299752\n",
      "1                   10  34774.194090\n",
      "2                   15  32895.099185\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping in `xgb.cv` is a technique used to prevent overfitting by halting the training process when the model's performance on a validation set stops improving. It monitors a specified evaluation metric and stops training if the metric does not improve for a given number of consecutive boosting rounds.\n",
    "\n",
    "### How Early Stopping Works in `xgb.cv`\n",
    "1. **Parameters**:\n",
    "   - `early_stopping_rounds`: The number of consecutive rounds without improvement after which training will be stopped.\n",
    "   - `metrics`: The evaluation metric to monitor (e.g., 'rmse', 'logloss').\n",
    "   - `as_pandas`: Whether to return the results as a pandas DataFrame.\n",
    "\n",
    "2. **Process**:\n",
    "   - During cross-validation, the model's performance is evaluated on a validation set at each boosting round.\n",
    "   - If the specified metric does not improve for `early_stopping_rounds` consecutive rounds, training is stopped.\n",
    "   - The best iteration is recorded, and the model parameters from that iteration are used.\n",
    "\n",
    "### Example Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0           1.464993        0.004004        1.467093       0.014044\n",
      "1           1.145017        0.003081        1.150520       0.013757\n",
      "2           0.938464        0.002520        0.948589       0.013294\n",
      "3           0.805468        0.001678        0.819693       0.012398\n",
      "4           0.718616        0.003191        0.734411       0.008979\n",
      "..               ...             ...             ...            ...\n",
      "212         0.316960        0.001958        0.475336       0.007646\n",
      "213         0.316600        0.002033        0.475354       0.007728\n",
      "214         0.316141        0.001964        0.475322       0.007779\n",
      "215         0.315654        0.001907        0.475273       0.007831\n",
      "216         0.315245        0.002167        0.475257       0.007612\n",
      "\n",
      "[217 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for training\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set parameters for the model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 4,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Perform cross-validation with early stopping\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=4,\n",
    "    metrics='rmse',\n",
    "    early_stopping_rounds=10,\n",
    "    as_pandas=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "- **Parameters**: Define the model parameters, including `objective`, `max_depth`, and `eval_metric`.\n",
    "- **Cross-Validation**: Perform cross-validation using `xgb.cv` with `early_stopping_rounds` set to 10. This means training will stop if the RMSE does not improve for 10 consecutive rounds.\n",
    "- **Results**: The results are returned as a pandas DataFrame, showing the evaluation metrics for each boosting round.\n",
    "\n",
    "This example demonstrates how to use early stopping in `xgb.cv` to prevent overfitting and find the optimal number of boosting rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tunable parameters\n",
    "\n",
    "**tree-related parameters**:\n",
    "\n",
    "* learning_rate\n",
    "* gamma \n",
    "* lambda \n",
    "* alpha \n",
    "* max_depth\n",
    "* subsample\n",
    "* colsamples_bytree\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunable Parameters for XGBoost Tree Base Learner\n",
    "\n",
    "1. **learning_rate (eta)**\n",
    "   - **Description**: Controls the step size at each boosting iteration. Lower values make the model more robust to overfitting but require more boosting rounds.\n",
    "   - **Typical Range**: 0.01 to 0.3\n",
    "   - **Default**: 0.3\n",
    "\n",
    "2. **gamma (min_split_loss)**\n",
    "   - **Description**: Minimum loss reduction required to make a further partition on a leaf node of the tree. Higher values make the algorithm more conservative.\n",
    "   - **Typical Range**: 0 to 5\n",
    "   - **Default**: 0\n",
    "\n",
    "3. **lambda (reg_lambda)**\n",
    "   - **Description**: L2 regularization term on weights. It helps prevent overfitting by shrinking the weights.\n",
    "   - **Typical Range**: 0 to 10\n",
    "   - **Default**: 1\n",
    "\n",
    "4. **alpha (reg_alpha)**\n",
    "   - **Description**: L1 regularization term on weights. It can help with feature selection by shrinking some feature weights to zero.\n",
    "   - **Typical Range**: 0 to 10\n",
    "   - **Default**: 0\n",
    "\n",
    "5. **max_depth**\n",
    "   - **Description**: Maximum depth of a tree. Increasing this value makes the model more complex and more likely to overfit.\n",
    "   - **Typical Range**: 3 to 10\n",
    "   - **Default**: 6\n",
    "\n",
    "6. **subsample**\n",
    "   - **Description**: Fraction of training samples used to grow each tree. Lower values prevent overfitting but may increase bias.\n",
    "   - **Typical Range**: 0.5 to 1\n",
    "   - **Default**: 1\n",
    "\n",
    "7. **colsample_bytree**\n",
    "   - **Description**: Fraction of features used to grow each tree. Lower values prevent overfitting but may increase bias.\n",
    "   - **Typical Range**: 0.5 to 1\n",
    "   - **Default**: 1\n",
    "\n",
    "### Example Code for Setting Parameters\n",
    "Below is an example of how to set these parameters in an XGBoost model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.22010881503525193\n",
      "Root Mean Squared Error: 0.46915755885976296\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for training\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set parameters for the model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "- **learning_rate**: Set to 0.1 to control the step size.\n",
    "- **gamma**: Set to 0.1 to make the algorithm more conservative.\n",
    "- **lambda**: Set to 1 for L2 regularization.\n",
    "- **alpha**: Set to 0 for no L1 regularization.\n",
    "- **max_depth**: Set to 6 to control the complexity of the trees.\n",
    "- **subsample**: Set to 0.8 to use 80% of the training samples for each tree.\n",
    "- **colsample_bytree**: Set to 0.8 to use 80% of the features for each tree.\n",
    "\n",
    "This example demonstrates how to set and tune the key parameters for an XGBoost tree-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear tunable parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Base Learner Parameters in XGBoost\n",
    "\n",
    "1. **lambda (reg_lambda)**\n",
    "   - **Description**: L2 regularization term on weights. It helps prevent overfitting by shrinking the weights.\n",
    "   - **Typical Range**: 0 to 10\n",
    "   - **Default**: 0\n",
    "\n",
    "2. **alpha (reg_alpha)**\n",
    "   - **Description**: L1 regularization term on weights. It can help with feature selection by shrinking some feature weights to zero.\n",
    "   - **Typical Range**: 0 to 10\n",
    "   - **Default**: 0\n",
    "\n",
    "3. **lambda_bias**\n",
    "   - **Description**: L2 regularization term on the bias. It helps prevent overfitting by shrinking the bias term.\n",
    "   - **Typical Range**: 0 to 10\n",
    "   - **Default**: 0\n",
    "\n",
    "### Summary\n",
    "- **lambda (reg_lambda)**: Controls L2 regularization on weights, reducing overfitting by shrinking weights.\n",
    "- **alpha (reg_alpha)**: Controls L1 regularization on weights, aiding feature selection by shrinking some weights to zero.\n",
    "- **lambda_bias**: Controls L2 regularization on the bias term, reducing overfitting by shrinking the bias.\n",
    "\n",
    "These parameters are crucial for tuning linear models in XGBoost to achieve better generalization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eta (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta  best_rmse\n",
      "0  0.001   1.931078\n",
      "1  0.010   1.793167\n",
      "2  0.100   0.974117\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(params=params, dtrain=housing_dmatrix, nfold=3, early_stopping_rounds=5, metrics='rmse', as_pandas=True, num_boost_round=10, seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth  best_rmse\n",
      "0          2   0.696380\n",
      "1          5   0.562417\n",
      "2         10   0.541528\n",
      "3         20   0.565039\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123, as_pandas=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree  best_rmse\n",
      "0               0.1   0.830366\n",
      "1               0.5   0.659547\n",
      "2               0.8   0.627300\n",
      "3               1.0   0.623250\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search and random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV vs RandomizedSearchCV\n",
    "\n",
    "#### GridSearchCV\n",
    "**What It Is**:\n",
    "- An exhaustive search method that evaluates all possible combinations of hyperparameters specified in a grid.\n",
    "- It systematically works through multiple combinations of parameter values, cross-validating as it goes to determine which combination gives the best performance.\n",
    "\n",
    "**Pros**:\n",
    "- **Comprehensive**: Evaluates all possible combinations, ensuring the best combination is found within the specified grid.\n",
    "- **Deterministic**: Always produces the same results given the same data and parameter grid.\n",
    "\n",
    "**Cons**:\n",
    "- **Computationally Expensive**: Can be very slow and resource-intensive, especially with a large number of parameters and values.\n",
    "- **Scalability**: Not practical for high-dimensional parameter spaces due to the exponential growth in the number of combinations.\n",
    "\n",
    "#### RandomizedSearchCV\n",
    "**What It Is**:\n",
    "- A search method that evaluates a fixed number of random combinations of hyperparameters from a specified distribution.\n",
    "- It samples a specified number of parameter settings from the given distributions and evaluates them.\n",
    "\n",
    "**Pros**:\n",
    "- **Efficiency**: Faster and less computationally expensive than GridSearchCV, especially with large parameter spaces.\n",
    "- **Scalability**: More practical for high-dimensional parameter spaces as it does not evaluate all combinations.\n",
    "- **Flexibility**: Allows specifying distributions for parameters, enabling more flexible and targeted searches.\n",
    "\n",
    "**Cons**:\n",
    "- **Non-Exhaustive**: Does not guarantee finding the best combination as it only evaluates a subset of possible combinations.\n",
    "- **Stochastic**: Results can vary between runs due to the random sampling of parameter combinations.\n",
    "\n",
    "### Summary\n",
    "- **GridSearchCV**: Exhaustive and deterministic but computationally expensive and less scalable.\n",
    "- **RandomizedSearchCV**: Efficient and scalable but non-exhaustive and stochastic.\n",
    "\n",
    "### Example Code\n",
    "Here is an example of how to use both methods with `XGBRegressor`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (GridSearchCV): {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}\n",
      "Best parameters (RandomizedSearchCV): {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters (GridSearchCV):\", grid_search.best_params_)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters (RandomizedSearchCV):\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example demonstrates how to set up and run both `GridSearchCV` and `RandomizedSearchCV` for hyperparameter tuning of an `XGBRegressor` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Lowest RMSE found:  29105.179169382693\n"
     ]
    }
   ],
   "source": [
    "# grid search example = ames\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "housing_data = pd.read_csv('ames_housing_trimmed_processed.csv')\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "gbm_param_grid = {'learning_rate': [0.01, 0.1, 0.5, 0.9], 'n_estimators': [200], 'subsample': [0.3, 0.5, 0.9]}\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "Best parameters found:  {'subsample': 0.8, 'n_estimators': 200, 'learning_rate': 0.05}\n",
      "Lowest RMSE found:  29420.3817978446\n"
     ]
    }
   ],
   "source": [
    "# random search example = ames\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "housing_data = pd.read_csv('ames_housing_trimmed_processed.csv')\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "gbm_param_grid = {'learning_rate': np.arange(0.05, 1.05, 0.05), 'n_estimators': [200], 'subsample': np.arange(0.05, 1.05, 0.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, n_iter=25, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 50}\n",
      "Lowest RMSE found:  0.706204362480588\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# \n",
    "# # Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 5}\n",
      "Lowest RMSE found:  0.6526003321681331\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, scoring='neg_mean_squared_error',\n",
    "n_iter=5, cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of GridSearchCV\n",
    "\n",
    "1. **Computationally Expensive**:\n",
    "   - **Description**: GridSearchCV evaluates all possible combinations of hyperparameters specified in the grid, which can be very slow and resource-intensive.\n",
    "   - **Impact**: This can be impractical for large datasets or models with many hyperparameters, leading to long training times and high computational costs.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - **Description**: The number of combinations grows exponentially with the number of hyperparameters and their possible values.\n",
    "   - **Impact**: This makes GridSearchCV less suitable for high-dimensional parameter spaces, as the search space can become prohibitively large.\n",
    "\n",
    "3. **Fixed Grid**:\n",
    "   - **Description**: GridSearchCV requires predefined parameter values, which may not cover the optimal values if the grid is not well-chosen.\n",
    "   - **Impact**: This can lead to suboptimal performance if the true optimal parameters lie between the specified grid points.\n",
    "\n",
    "4. **Overfitting Risk**:\n",
    "   - **Description**: Evaluating many combinations increases the risk of overfitting to the validation set used during cross-validation.\n",
    "   - **Impact**: This can result in a model that performs well on the validation set but poorly on unseen data.\n",
    "\n",
    "### Limitations of RandomizedSearchCV\n",
    "\n",
    "1. **Non-Exhaustive Search**:\n",
    "   - **Description**: RandomizedSearchCV evaluates a fixed number of random combinations of hyperparameters, which means it does not cover the entire search space.\n",
    "   - **Impact**: There is no guarantee that the best combination of hyperparameters will be found, especially if the number of iterations is low.\n",
    "\n",
    "2. **Stochastic Nature**:\n",
    "   - **Description**: The results can vary between runs due to the random sampling of parameter combinations.\n",
    "   - **Impact**: This can lead to variability in the selected hyperparameters and model performance, requiring multiple runs to ensure stability.\n",
    "\n",
    "3. **Requires Distribution Knowledge**:\n",
    "   - **Description**: RandomizedSearchCV requires specifying distributions for the hyperparameters, which may not be straightforward for all parameters.\n",
    "   - **Impact**: Poorly chosen distributions can lead to inefficient searches and suboptimal performance.\n",
    "\n",
    "4. **Computational Cost**:\n",
    "   - **Description**: While generally more efficient than GridSearchCV, RandomizedSearchCV can still be computationally expensive if the number of iterations is high or if the model is complex.\n",
    "   - **Impact**: This can limit its practicality for very large datasets or highly complex models.\n",
    "\n",
    "### Summary\n",
    "- **GridSearchCV**: Comprehensive but computationally expensive and less scalable. Fixed grid may miss optimal values and increases overfitting risk.\n",
    "- **RandomizedSearchCV**: More efficient and scalable but non-exhaustive and stochastic. Requires careful distribution specification and can still be computationally costly.\n",
    "\n",
    "Both methods have their strengths and weaknesses, and the choice between them depends on the specific requirements and constraints of the problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
