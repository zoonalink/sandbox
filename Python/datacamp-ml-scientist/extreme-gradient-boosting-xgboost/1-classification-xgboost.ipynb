{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC stands for \"Area Under the Curve.\" It is a performance measurement for classification problems at various threshold settings. AUC represents the degree or measure of separability achieved by the model. It tells how much the model is capable of distinguishing between classes.\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "1. **ROC Curve**: The AUC is derived from the ROC (Receiver Operating Characteristic) curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "2. **True Positive Rate (TPR)**: Also known as recall or sensitivity, it is the ratio of correctly predicted positive observations to all actual positives. TPR = TP / (TP + FN).\n",
    "\n",
    "3. **False Positive Rate (FPR)**: It is the ratio of incorrectly predicted positive observations to all actual negatives. FPR = FP / (FP + TN).\n",
    "\n",
    "4. **AUC Value**: The AUC value ranges from 0 to 1. A higher AUC value indicates a better performing model.\n",
    "   - **AUC = 1**: Perfect model.\n",
    "   - **0.5 < AUC < 1**: Good model.\n",
    "   - **AUC = 0.5**: Model with no discrimination capability (random guessing).\n",
    "   - **AUC < 0.5**: Poor model (worse than random guessing).\n",
    "\n",
    "5. **Interpretation**: AUC provides an aggregate measure of performance across all classification thresholds. It is useful for comparing different models.\n",
    "\n",
    "In summary, AUC is a valuable metric for evaluating the performance of a classification model, especially when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Overview\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is a powerful and efficient implementation of the gradient boosting framework. It is widely used for supervised learning tasks, especially for classification and regression problems.\n",
    "\n",
    "### How XGBoost Works\n",
    "\n",
    "1. **Boosting**: XGBoost builds an ensemble of trees sequentially. Each tree tries to correct the errors of the previous one.\n",
    "2. **Gradient Descent**: It uses gradient descent to minimize the loss function, improving the model iteratively.\n",
    "3. **Regularization**: XGBoost includes regularization terms to prevent overfitting, making it more robust than other boosting algorithms.\n",
    "4. **Tree Pruning**: It uses a technique called \"max depth\" to limit the growth of trees, which helps in controlling overfitting.\n",
    "5. **Handling Missing Values**: XGBoost can handle missing values internally, which makes it very flexible.\n",
    "\n",
    "### Pros of XGBoost\n",
    "\n",
    "1. **High Performance**: XGBoost is known for its high performance and speed. It is optimized for both memory and computation.\n",
    "2. **Regularization**: It includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
    "3. **Flexibility**: Supports various objective functions and evaluation metrics.\n",
    "4. **Handling Missing Data**: Automatically handles missing values.\n",
    "5. **Parallel Processing**: Supports parallel processing, making it faster on large datasets.\n",
    "6. **Cross-Validation**: Built-in cross-validation capabilities.\n",
    "7. **Feature Importance**: Provides feature importance scores, which help in feature selection.\n",
    "\n",
    "### Cons of XGBoost\n",
    "\n",
    "1. **Complexity**: XGBoost can be complex to tune due to the large number of hyperparameters.\n",
    "2. **Computationally Intensive**: Despite being optimized, it can still be computationally intensive, especially on very large datasets.\n",
    "3. **Overfitting**: If not properly tuned, XGBoost can overfit, especially on small datasets.\n",
    "4. **Interpretability**: Models can be less interpretable compared to simpler models like linear regression.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Here's a simple example of how to use XGBoost in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example demonstrates loading a dataset, splitting it into training and testing sets, training an XGBoost classifier, making predictions, and evaluating the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example xgboost model for classification\n",
    "\n",
    "# load data\n",
    "class_data = pd.read_csv(\"classifiation.csv\")\n",
    "\n",
    "# split data into X and y\n",
    "X, y = class_data.iloc[:, :-1], class_data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# xgboost model\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "preds = xg_cl.predict(X_test)\n",
    "accuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\n",
    "\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees as Base Learners\n",
    "\n",
    "In ensemble methods like XGBoost, decision trees are often used as base learners. A decision tree is a flowchart-like structure where each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an outcome (or class label).\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Non-Parametric**: Decision trees do not assume any underlying distribution of the data.\n",
    "- **Recursive Partitioning**: The dataset is split into subsets based on the feature that provides the best split according to a certain criterion (e.g., Gini impurity, entropy).\n",
    "- **Interpretability**: Decision trees are easy to interpret and visualize.\n",
    "\n",
    "### CART (Classification and Regression Trees)\n",
    "\n",
    "CART is a specific type of decision tree algorithm introduced by Breiman et al. in 1984. It can be used for both classification and regression tasks.\n",
    "\n",
    "**How CART Works:**\n",
    "1. **Splitting**: The algorithm splits the data into two subsets at each node based on a feature and a threshold that minimize a certain cost function (e.g., Gini impurity for classification, mean squared error for regression).\n",
    "2. **Recursive Process**: This process is repeated recursively for each subset until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
    "3. **Pruning**: To avoid overfitting, CART can prune the tree by removing branches that have little importance.\n",
    "\n",
    "**Key Features:**\n",
    "- **Binary Splits**: CART always produces binary trees (each node has two children).\n",
    "- **Gini Impurity**: For classification tasks, CART often uses Gini impurity to measure the quality of a split.\n",
    "- **Mean Squared Error**: For regression tasks, CART uses mean squared error to measure the quality of a split.\n",
    "\n",
    "### Example of a Decision Tree in Python\n",
    "\n",
    "Here's a simple example using the `DecisionTreeClassifier` from `scikit-learn`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(max_depth=4,random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example demonstrates loading a dataset, splitting it into training and testing sets, training a decision tree classifier, making predictions, and evaluating the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting in XGBoost\n",
    "\n",
    "Boosting is a meta-algorithm that combines multiple weak learners to create a strong learner. In the context of XGBoost, the weak learners are typically decision trees. The main idea is to sequentially add trees to the model, each one correcting the errors of the previous ones.\n",
    "\n",
    "### How Boosting Works in XGBoost\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction, often the mean of the target values for regression or the log-odds for classification.\n",
    "2. **Sequential Learning**: Add decision trees one by one. Each new tree is trained to predict the residual errors (the difference between the actual and predicted values) of the previous trees.\n",
    "3. **Weighted Sum**: Combine the predictions of all trees using a weighted sum. The weights are determined by the learning rate.\n",
    "4. **Regularization**: Apply regularization to prevent overfitting.\n",
    "\n",
    "### Meta-Algorithm: Gradient Boosting\n",
    "\n",
    "Gradient Boosting is the meta-algorithm used in XGBoost. It minimizes a loss function by adding weak learners (decision trees) in a sequential manner.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DMatrix` is a data structure used in XGBoost to optimize data handling and computation. It is designed to be efficient in terms of both memory and computation, especially for large datasets.\n",
    "\n",
    "### Key Features of `DMatrix`\n",
    "\n",
    "1. **Efficient Storage**: `DMatrix` stores data in a compressed and efficient format, which speeds up the training process.\n",
    "2. **Handling Missing Values**: It can handle missing values efficiently.\n",
    "3. **Support for Sparse Data**: `DMatrix` can handle sparse data formats, which is useful for datasets with many zero entries.\n",
    "4. **Meta Information**: It can store additional information such as labels, weights, and base margins.\n",
    "\n",
    "### Creating a `DMatrix`\n",
    "\n",
    "You can create a `DMatrix` from various data sources such as NumPy arrays, Pandas DataFrames, or even from files.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Here's an example of how to use `DMatrix` with the breast cancer dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for training and testing sets\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Train the model using DMatrix\n",
    "num_rounds = 100\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Load Dataset**: Load the breast cancer dataset.\n",
    "2. **Split Data**: Split the data into training and testing sets.\n",
    "3. **Create DMatrix**: Create `DMatrix` objects for the training and testing sets.\n",
    "4. **Set Parameters**: Define the parameters for the XGBoost model.\n",
    "5. **Train Model**: Train the model using the `train` function and the `DMatrix` for the training set.\n",
    "6. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
    "7. **Evaluate Accuracy**: Evaluate and print the model's accuracy on the test set.\n",
    "\n",
    "Using `DMatrix` helps in optimizing the performance of XGBoost, especially when dealing with large datasets.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost Classifier\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Initialization**: The model starts with an initial prediction.\n",
    "2. **Sequential Learning**: The `n_estimators` parameter specifies the number of trees to be added sequentially. Each tree corrects the errors of the previous trees.\n",
    "3. **Learning Rate**: The `learning_rate` parameter controls the contribution of each tree. A lower learning rate requires more trees but can lead to better generalization.\n",
    "4. **Objective**: The `objective` parameter specifies the loss function to be minimized. For binary classification, `binary:logistic` is used.\n",
    "\n",
    "This example demonstrates how XGBoost uses boosting to combine multiple weak learners (decision trees) into a strong learner, improving the model's performance.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using cv instead of train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, instead of calling the `train` method, you can use the `cv` method in XGBoost to perform cross-validation. The `cv` method provides a way to evaluate the model's performance using cross-validation, which can help in tuning hyperparameters and assessing the model's generalization ability.\n",
    "\n",
    "### Benefits of Using `cv` Method\n",
    "\n",
    "1. **Cross-Validation**: Automatically performs k-fold cross-validation, providing a more robust estimate of the model's performance.\n",
    "2. **Hyperparameter Tuning**: Helps in tuning hyperparameters by evaluating different configurations and selecting the best one based on cross-validation results.\n",
    "3. **Early Stopping**: Supports early stopping, which can prevent overfitting by stopping the training process when the performance on the validation set stops improving.\n",
    "\n",
    "### Example Usage of `cv` Method\n",
    "\n",
    "Here's an example of using the `cv` method with the breast cancer dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-logloss-mean  train-logloss-std  test-logloss-mean  test-logloss-std\n",
      "0             0.611042           0.000846           0.619844          0.002112\n",
      "1             0.542897           0.001865           0.558142          0.001745\n",
      "2             0.484471           0.002167           0.506111          0.002396\n",
      "3             0.435159           0.002295           0.462051          0.005074\n",
      "4             0.392218           0.002758           0.425952          0.006387\n",
      "..                 ...                ...                ...               ...\n",
      "95            0.012397           0.000945           0.127571          0.050083\n",
      "96            0.012261           0.000921           0.127939          0.050013\n",
      "97            0.012156           0.000906           0.127920          0.049876\n",
      "98            0.012055           0.000880           0.127764          0.050011\n",
      "99            0.011947           0.000850           0.127506          0.050235\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for training set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    nfold=5,\n",
    "    metrics={'logloss'},\n",
    "    early_stopping_rounds=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(cv_results)\n",
    "\n",
    "# Train the model using the best number of boosting rounds from cross-validation\n",
    "best_num_boost_round = cv_results.shape[0]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=best_num_boost_round)\n",
    "\n",
    "# Create DMatrix for testing set\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Load Dataset**: Load the breast cancer dataset.\n",
    "2. **Split Data**: Split the data into training and testing sets.\n",
    "3. **Create DMatrix**: Create a `DMatrix` object for the training set.\n",
    "4. **Set Parameters**: Define the parameters for the XGBoost model.\n",
    "5. **Cross-Validation**: Use the `cv` method to perform 5-fold cross-validation. The `early_stopping_rounds` parameter stops training if the performance does not improve for 10 consecutive rounds.\n",
    "6. **Print Results**: Print the cross-validation results.\n",
    "7. **Train Model**: Train the model using the best number of boosting rounds determined by cross-validation.\n",
    "8. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
    "9. **Evaluate Accuracy**: Evaluate and print the model's accuracy on the test set.\n",
    "\n",
    "### Difference Between `train` and `cv`\n",
    "\n",
    "- **`train` Method**: Trains the model on the entire training set without cross-validation. It is faster but may not provide a robust estimate of the model's performance.\n",
    "- **`cv` Method**: Performs cross-validation, providing a more reliable estimate of the model's performance and helping in hyperparameter tuning. It can also use early stopping to prevent overfitting.\n",
    "\n",
    "Using the `cv` method is beneficial when you want to evaluate the model's performance more rigorously and tune hyperparameters effectively.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using 'error' instead of 'logloss' as the evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, if you use `'error'` instead of `'logloss'` and set `as_pandas=True`, the \n",
    "\n",
    "cv_results\n",
    "\n",
    " will contain the training and testing error mean and standard deviation. The `'error'` metric in XGBoost represents the classification error rate, which is the fraction of incorrect predictions.\n",
    "\n",
    "Here's how you can modify your code to use `'error'` and calculate the accuracy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0           0.030769         0.003204         0.094505        0.014906\n",
      "1           0.027473         0.004914         0.083516        0.011207\n",
      "2           0.023626         0.005095         0.072527        0.016447\n",
      "3           0.023077         0.004790         0.072527        0.023671\n",
      "4           0.021429         0.003204         0.070330        0.023671\n",
      "5           0.021429         0.003204         0.072527        0.022628\n",
      "6           0.018132         0.003297         0.072527        0.020382\n",
      "7           0.018132         0.004464         0.065934        0.023051\n",
      "8           0.016484         0.003475         0.072527        0.020382\n",
      "9           0.014835         0.003297         0.072527        0.022628\n",
      "10          0.014835         0.004112         0.076923        0.019658\n",
      "11          0.013736         0.003009         0.074725        0.018906\n",
      "12          0.013187         0.003204         0.072527        0.016447\n",
      "13          0.012637         0.002802         0.070330        0.016447\n",
      "14          0.010989         0.003885         0.072527        0.019160\n",
      "15          0.010989         0.003885         0.065934        0.018388\n",
      "16          0.010989         0.003885         0.070330        0.020382\n",
      "17          0.009890         0.003727         0.065934        0.018388\n",
      "18          0.008791         0.003645         0.065934        0.018388\n",
      "19          0.007692         0.003645         0.065934        0.018388\n",
      "20          0.007692         0.003645         0.065934        0.018388\n",
      "21          0.007692         0.003645         0.065934        0.015541\n",
      "22          0.007143         0.002802         0.065934        0.018388\n",
      "23          0.007143         0.002802         0.065934        0.018388\n",
      "24          0.007143         0.002802         0.063736        0.016150\n",
      "25          0.007143         0.002802         0.063736        0.017582\n",
      "26          0.007143         0.002802         0.065934        0.015541\n",
      "27          0.006593         0.002802         0.061538        0.017855\n",
      "28          0.006593         0.002802         0.061538        0.017855\n",
      "29          0.007143         0.003297         0.061538        0.017855\n",
      "30          0.006044         0.002056         0.059341        0.019160\n",
      "31          0.006044         0.002056         0.059341        0.019160\n",
      "32          0.006044         0.002056         0.057143        0.021308\n",
      "33          0.006044         0.002056         0.054945        0.019658\n",
      "34          0.006044         0.002056         0.057143        0.021308\n",
      "35          0.006044         0.002056         0.057143        0.021308\n",
      "36          0.006044         0.002056         0.057143        0.021308\n",
      "37          0.005495         0.001738         0.057143        0.021308\n",
      "38          0.004945         0.002692         0.054945        0.019658\n",
      "39          0.004945         0.002692         0.057143        0.021308\n",
      "40          0.004945         0.002692         0.050549        0.020382\n",
      "41          0.004396         0.002198         0.052747        0.018906\n",
      "42          0.004396         0.002198         0.052747        0.018906\n",
      "43          0.003846         0.002198         0.050549        0.020382\n",
      "44          0.003846         0.002198         0.052747        0.018906\n",
      "45          0.003846         0.002198         0.050549        0.014906\n",
      "46          0.003846         0.002198         0.048352        0.011207\n",
      "47          0.003846         0.002198         0.048352        0.011207\n",
      "48          0.003297         0.002056         0.048352        0.011207\n",
      "49          0.002747         0.002457         0.048352        0.011207\n",
      "50          0.002198         0.002056         0.048352        0.011207\n",
      "51          0.002198         0.002056         0.048352        0.008791\n",
      "52          0.002198         0.002056         0.050549        0.011207\n",
      "53          0.002198         0.002056         0.048352        0.011207\n",
      "54          0.001648         0.002198         0.046154        0.008223\n",
      "55          0.001648         0.002198         0.050549        0.011207\n",
      "56          0.001648         0.002198         0.043956        0.012038\n",
      "Train Accuracy: 0.9983516483516484\n",
      "Test Accuracy: 0.9560439560439561\n",
      "Accuracy on test set: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for training set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    nfold=5,\n",
    "    metrics={'error'},\n",
    "    early_stopping_rounds=10,\n",
    "    seed=42,\n",
    "    as_pandas=True\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(cv_results)\n",
    "\n",
    "# Calculate accuracy from error\n",
    "train_error_mean = cv_results['train-error-mean'].iloc[-1]\n",
    "test_error_mean = cv_results['test-error-mean'].iloc[-1]\n",
    "train_accuracy = 1 - train_error_mean\n",
    "test_accuracy = 1 - test_error_mean\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Train the model using the best number of boosting rounds from cross-validation\n",
    "best_num_boost_round = cv_results.shape[0]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=best_num_boost_round)\n",
    "\n",
    "# Create DMatrix for testing set\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Change Metric**: Set `eval_metric` to `'error'` to use the classification error rate.\n",
    "2. **as_pandas=True**: Set `as_pandas=True` to get the cross-validation results as a Pandas DataFrame.\n",
    "3. **Calculate Accuracy**: Calculate the training and testing accuracy from the error mean values.\n",
    "4. **Print Results**: Print the training and testing accuracy.\n",
    "5. **Train Model**: Train the model using the best number of boosting rounds determined by cross-validation.\n",
    "6. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
    "7. **Evaluate Accuracy**: Evaluate and print the model's accuracy on the test set.\n",
    "\n",
    "By using `'error'` as the evaluation metric, you can directly calculate the accuracy from the error rates provided in the cross-validation results.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to use XGBoost\n",
    "\n",
    "* large number of training samples\n",
    "  * greater than 1000 training samples and less than 100 features\n",
    "  * number of features < number of training samples\n",
    "* mixture of categorical and numerical features\n",
    "  * or just numberic\n",
    "\n",
    "### when NOT to use\n",
    "\n",
    "* image recognition\n",
    "* computer vision\n",
    "* natural language processing and understanding problems\n",
    "* small training samples (less than 100 samples)\n",
    "* number of features > number of training samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
