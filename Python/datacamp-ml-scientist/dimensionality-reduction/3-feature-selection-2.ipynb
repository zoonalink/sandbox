{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction 2 - model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) is a feature selection technique that recursively removes the least important features and builds a model using the remaining features. It ranks the features by importance and selects the best subset of features for model building.\n",
    "\n",
    "Here's a brief overview of how to use RFE in Python with `scikit-learn`:\n",
    "\n",
    "1. **Import necessary libraries:**\n",
    "   ```python\n",
    "   from sklearn.feature_selection import RFE\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   ```\n",
    "\n",
    "2. **Create the model and RFE object:**\n",
    "   ```python\n",
    "   model = LogisticRegression()\n",
    "   rfe = RFE(model, n_features_to_select=5)\n",
    "   ```\n",
    "\n",
    "3. **Fit the RFE object to the data:**\n",
    "   ```python\n",
    "   rfe = rfe.fit(X, y)\n",
    "   ```\n",
    "\n",
    "4. **Use `rfe.support_` to get a boolean mask of the selected features:**\n",
    "   ```python\n",
    "   selected_features = rfe.support_\n",
    "   ```\n",
    "\n",
    "5. **Use `rfe.ranking_` to get the ranking of all features:**\n",
    "   ```python\n",
    "   feature_ranking = rfe.ranking_\n",
    "   ```\n",
    "\n",
    "### Example Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [ True  True  True  True False False  True False False False]\n",
      "Feature Ranking: [1 1 1 1 3 4 1 5 2 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Create a model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the RFE object\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE object to the data\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# Get the boolean mask of selected features\n",
    "selected_features = rfe.support_\n",
    "\n",
    "# Get the ranking of all features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Feature Ranking:\", feature_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- `rfe.support_`: A boolean array indicating which features are selected.\n",
    "- `rfe.ranking_`: An array of feature rankings, where 1 indicates the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking: {'feature_0': 1, 'feature_1': 1, 'feature_2': 1, 'feature_3': 1, 'feature_4': 3, 'feature_5': 4, 'feature_6': 1, 'feature_7': 5, 'feature_8': 2, 'feature_9': 6}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])\n",
    "\n",
    "# Create a model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the RFE object\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE object to the data\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# Get the ranking of all features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Create a dictionary mapping feature names to their rankings\n",
    "feature_ranking_dict = dict(zip(X.columns, feature_ranking))\n",
    "\n",
    "print(\"Feature Ranking:\", feature_ranking_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes classifier\n",
    "\n",
    "Using Pima Indians dataset to predict diabetes using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import PimaIndians.csv\n",
    "pima = pd.read_csv('PimaIndians.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnant  glucose  diastolic  triceps  insulin   bmi  family  age      test\n",
      "0         1       89         66       23       94  28.1   0.167   21  negative\n",
      "1         0      137         40       35      168  43.1   2.288   33  positive\n",
      "2         3       78         50       32       88  31.0   0.248   26  positive\n",
      "3         2      197         70       45      543  30.5   0.158   53  positive\n",
      "4         1      189         60       23      846  30.1   0.398   59  positive\n"
     ]
    }
   ],
   "source": [
    "print(pima.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = pima.drop('test', axis=1)\n",
    "y = pima['test']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.2% accuracy on test set.\n",
      "{'pregnant': 0.36, 'glucose': 1.12, 'diastolic': 0.13, 'triceps': 0.23, 'insulin': 0.13, 'bmi': 0.32, 'family': 0.4, 'age': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# fit scaler and transform training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# fit model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scale test data and make predictions\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# predict\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "#  accuracy metrics and feature coefficients\n",
    "print(f\"{accuracy_score(y_test, y_pred):.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model be improved by reducing the number of features without hurting the model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n",
      "{'pregnant': 0.05, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.35, 'age': 0.34}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = pima[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove diastolic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.6% accuracy on test set.\n",
      "{'pregnant': 0.05, 'glucose': 1.24, 'triceps': 0.24, 'insulin': 0.2, 'bmi': 0.39, 'family': 0.34, 'age': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = pima[[ 'pregnant','glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove two more features - pregnant, insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n",
      "{'glucose': 1.13, 'triceps': 0.25, 'bmi': 0.34, 'family': 0.34, 'age': 0.37}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = pima[['glucose', 'triceps',  'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only highest coef feature - glucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.5% accuracy on test set.\n",
      "{'glucose': 1.28}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = pima[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n",
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
      "80.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = pima[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "# scale train, test data \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test_scaled))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlscientist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
