{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "* Mean Absolute Error (MAE): The average difference between predicted values and true values. This value is based on the same units as the label, in this case dollars. The lower this value is, the better the model is predicting.\n",
    "* Root Mean Squared Error (RMSE): The square root of the mean squared difference between predicted and true values. The result is a metric based on the same unit as the label (dollars). When compared to the MAE (above), a larger difference indicates greater variance in the individual errors (for example, with some errors being very small, while others are large).\n",
    "* Relative Squared Error (RSE): A relative metric between 0 and 1 based on the square of the differences between predicted and true values. The closer to 0 this metric is, the better the model is performing. Because this metric is relative, it can be used to compare models where the labels are in different units.\n",
    "* Relative Absolute Error (RAE): A relative metric between 0 and 1 based on the absolute differences between predicted and true values. The closer to 0 this metric is, the better the model is performing. Like RSE, this metric can be used to compare models where the labels are in different units.\n",
    "* Coefficient of Determination (R2): This metric is more commonly referred to as R-Squared, and summarizes how much of the variance between predicted and true values is explained by the model. The closer to 1 this value is, the better the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipeline](https://ml.azure.com/experiments/id/f5214921-48e4-4fdb-a398-32ae664b65a2/runs/ad5ffb70-2032-47c9-8a5e-fbd3cb926a6f?wsid=/subscriptions/b106f384-f423-441b-916e-039f08508fe2/resourcegroups/petter2_ML/providers/Microsoft.MachineLearningServices/workspaces/petter2_ML&tid=07ef1208-413c-4b5e-9cdd-64ef305754f0#/?graphId=076d9f3d-0d71-485f-99d8-6f71299f2103&label=Auto+Price+Training&newGraphId=076d9f3d-0d71-485f-99d8-6f71299f2103&path=%2Fexperiments%2Fid%2Ff5214921-48e4-4fdb-a398-32ae664b65a2%2Fruns%2Fad5ffb70-2032-47c9-8a5e-fbd3cb926a6f&runId=ad5ffb70-2032-47c9-8a5e-fbd3cb926a6f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipeline with python](https://ml.azure.com/experiments/id/8f4f39bd-68a4-43aa-89f1-555bb878ceab/runs/72084f3c-62b5-4d23-95b1-6a168c85b4a2?wsid=/subscriptions/b106f384-f423-441b-916e-039f08508fe2/resourcegroups/petter2_ML/providers/Microsoft.MachineLearningServices/workspaces/petter2_ML&tid=07ef1208-413c-4b5e-9cdd-64ef305754f0#/?graphId=f65becf5-ee2c-4f25-b0a6-22d89de8c4b6&label=mslearn-auto-inference&newGraphId=f65becf5-ee2c-4f25-b0a6-22d89de8c4b6&path=%2Fexperiments%2Fid%2F8f4f39bd-68a4-43aa-89f1-555bb878ceab%2Fruns%2F72084f3c-62b5-4d23-95b1-6a168c85b4a2&runId=72084f3c-62b5-4d23-95b1-6a168c85b4a2)\n",
    "\n",
    "![](2023-04-09-18-53-15.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-04-09-18-55-08.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification notes\n",
    "\n",
    "### Confusion matrix\n",
    "The confusion matrix is a tool used to assess the quality of a classification model's predictions. It compares predicted labels against actual labels.\n",
    "\n",
    "In a binary classification model where you're predicting one of two possible values, the confusion matrix is a 2x2 grid showing the predicted and actual value counts for classes 1 and 0. It categorizes the model's results into four types of outcomes. Using our diabetes example these outcomes can look like:\n",
    "\n",
    "* True Positive: The model predicts the patient has diabetes, and the patient does actually have diabetes.\n",
    "* False Positive: The model predicts the patient has diabetes, but the patient does not actually have diabetes.\n",
    "* False Negative: The model predicts the patient does not have diabetes, but the patient actually does have diabetes.\n",
    "* True Negative: The model predicts the patient does not have diabetes, and the patient actually does not have diabetes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multi-class classification model (where there are more than two possible classes), the same approach is used to tabulate each possible combination of actual and predicted value counts - so a model with three possible classes would result in a 3x3 matrix with a diagonal line of cells where the predicted and actual labels match.\n",
    "\n",
    "Metrics that can be derived from the confusion matrix include:\n",
    "\n",
    "* Accuracy: The number of correct predictions (true positives + true negatives) divided by the total number of predictions.\n",
    "* Precision: The number of the cases classified as positive that are actually positive: the number of true positives divided by (the number of true positives plus false positives).\n",
    "* Recall: The fraction of positive cases correctly identified: the number of true positives divided by (the number of true positives plus false negatives).\n",
    "* F1 Score: An overall metric that essentially combines precision and recall.\n",
    "* \n",
    "Of these metric, accuracy may be the most intuitive. However, you need to be careful about using accuracy as a measurement of how well a model performs. Using the model that predicts 15% of patients have diabetes, when actually 25% of patients have diabetes, we can calculate the following metrics:\n",
    "\n",
    "The accuracy of the model is: (10+70)/ 100 = 80%.\n",
    "\n",
    "The precision of the model is: 10/(10+5) = 67%.\n",
    "\n",
    "The recall of the model is 10/(10+15) = 40%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "A classification model predicts the probability for each possible class. In other words, the model calculates a likelihood for each predicted label. In the case of a binary classification model, the predicted probability is a value between 0 and 1. By default, a predicted probability including or above 0.5 results in a class prediction of 1, while a prediction below this threshold means that there's a greater probability of a negative prediction (remember that the probabilities for all classes add up to 1), so the predicted class would be 0.\n",
    "\n",
    "Designer has a useful threshold slider for reviewing how the model performance would change depending on the set threshold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve and AUC metric\n",
    "\n",
    "Another term for recall is True positive rate, and it has a corresponding metric named False positive rate, which measures the number of negative cases incorrectly identified as positive compared the number of actual negative cases. \n",
    "\n",
    "Plotting these metrics against each other for every possible threshold value between 0 and 1 results in a curve, known as the ROC curve (ROC stands for receiver operating characteristic, but most data scientists just call it a ROC curve). \n",
    "\n",
    "In an ideal model, the curve would go all the way up the left side and across the top, so that it covers the full area of the chart. The larger the area under the curve, of AUC metric, (which can be any value from 0 to 1), the better the model is performing. You can review the ROC curve in Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
